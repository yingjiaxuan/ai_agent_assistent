
Designing Prompts and Multi-Layered Memory Architecture Toward AI Agents for Cross-Lingual Support


Abstract: Large Language Models (LLMs) face significant limitations in extended dialogues and cross-session interactions due to fixed context windows: without long-term memory, they tend to forget information, cannot continuously learn from past interactions, and may lose track of objectives during lengthy tasks. This issue is exacerbated in cross-lingual scenarios. Even though modern LLMs are often multilingual, they struggle to correlate knowledge across languages, revealing a clear cross-lingual knowledge barrier. To address these challenges, this paper presents mid-term research on an AI agent that integrates prompt design with a multi-layered memory architecture for cross-lingual support. The proposed system incorporates three memory layers – Short-Term Memory (STM), Long-Term Memory (LTM), and User Profile (UP) – enabling the agent to retain persona information, user preferences, and dialogue context across turns and languages. We describe the background and design of this architecture, detail its implementation, and evaluate it on benchmark long-conversation tasks (LoCoMo) and custom scenario scripts. The results demonstrate that the multi-layer memory approach significantly improves character memory retention, preference adherence, and cross-turn consistency. In our experiments, the full system improves QA accuracy by about 20–30% over a no-memory baseline and markedly reduces hallucination rate, while maintaining reasonable latency. These findings indicate the effectiveness of combining prompt engineering with hierarchical memory for creating more consistent and cross-lingually capable AI agents.

Keywords: Prompt Engineering; Multi-layer Memory; Cross-lingual Support; Conversational AI; Long-term Dialogue


I. Introduction and Background

Contemporary LLM-based chatbots struggle with long-term context retention and multilingual consistency in extended conversations. Although LLMs trained on massive multilingual data have some cross-lingual capability, their internal lack of explicit long-term memory causes them to forget previously provided information as dialogue sessions grow or span longer periods, leading to incoherent or self-contradictory responses. Moreover, LLMs’ ability to share knowledge across different languages is limited: they often fail at tasks requiring implicit cross-lingual reasoning, revealing a clear “cross-lingual knowledge gap.” These limitations make current AI dialogue agents unsatisfactory in applications that require long-term memory and multilingual consistency (such as cross-lingual assistants or bilingual customer service). To mitigate the fixed context window of LLMs, recent research has proposed expanding the model’s context or introducing external memory. One approach is long-context models, which increase the attention window or use efficient transformer variants to accommodate more dialogue history at once. However, even with extended context (tens of thousands of tokens), LLMs still struggle to understand very long conversations or perform long-range reasoning effectively, falling short of human performance. Another approach is retrieval-augmented generation (RAG), which stores dialogue history in an external knowledge base and retrieves relevant pieces for the model to use when generating a response. This method can significantly improve an LLM’s ability to recall long-term dialogue knowledge, yielding substantial performance gains in tasks like open-domain QA (reported improvements of roughly 22–66%). However, simply having a retrieval tool does not fully solve context management: the agent must decide when to retrieve and how to incorporate the retrieved information, or even the most powerful retrieval system will not help. In recent years, hierarchical memory architectures have been proposed. For example, MemGPT introduced an OS-inspired memory hierarchy, using paging to divide memory into a “main context” (like RAM) and an “external context” (like disk), overcoming the fixed-window limitation. Building on this idea, open-source frameworks like mem0 have implemented multi-level memory units (user-level, session-level, agent-level) to adaptively store user preferences, dialogue state, etc. These systems combine short-term memory with long-term storage and have shown excellent consistency in long conversations. For instance, mem0 reports a 26% accuracy improvement on the LoCoMo long-dialogue QA benchmark over OpenAI’s native memory, while reducing per-response latency by 91% and context token usage by 90%. Additionally, in multilingual settings, studies have shown that using multilingual embeddings to map different languages into a shared vector space enables efficient cross-lingual retrieval. For example, a user query in English can retrieve relevant segments from stored French content. This makes cross-lingual long-term memory feasible. However, there is still a lack of memory architectures specifically designed for cross-lingual dialogue that integrate these ideas into a practical dialogue agent. Therefore, our system is built on a three-layer structure of short-term memory, long-term memory, and user profile, exploring effective prompt design and memory coordination for cross-lingual conversation. In the following, we detail the architecture design, system implementation, experimental evaluation, and discuss future work.


II. Related Work

Extended Contexts vs External Memory

To enable LLMs to remember in long conversations, some research has extended the transformer context window (e.g., to 8K, 32K tokens or more) using efficient variants like Longformer or Transformer-XL. However, these approaches greatly increase computational cost, and studies find that simply providing more context does not guarantee the model will use it effectively. Therefore, another direction uses external tools or databases to achieve “infinite memory.” Retrieval-augmented generation (RAG) is a typical example: models like REALM and RETRO store knowledge in vectorized form and retrieve relevant passages to generate answers. This method works well in open-domain QA. Yet in dialogue settings, directly applying RAG has challenges: the agent must decide when to retrieve and how to incorporate the results. If prompts are not designed properly or training lacks examples, the agent may fail to use the retrieval tool effectively.

Hierarchical Memory Architectures

Inspired by operating system memory hierarchies, MemGPT first explored enabling LLMs to manage a virtual long-term memory. It allows an LLM to read and write external storage via function calls, implementing a “paging” mechanism within a limited context window. Specifically, MemGPT divides memory into a primary context (analogous to RAM) and an external context (analogous to disk). When the primary context is full, portions of it are written to external storage and later read back as needed. This OS-style memory management lets the agent handle long documents and multi-session chats effectively. Building on this, industry frameworks like mem0 have implemented multi-level memory pools that combine short-term conversation content with a long-term knowledge base. The mem0 system introduces modules such as a message router, prompt processor, and memory engine to capture dialogue threads, store structured memory objects (with embeddings), and fetch relevant content on demand. It supports multiple memory types (facts, events, intents) and handles multi-threaded conversations, user preferences, and context compression. These designs significantly improve an agent’s consistency in long-running dialogues and context management. Another open-source approach, Memori, proposed a dual-mode memory injection: an explicit short-term memory that is injected at the start of a conversation (containing recent important information), and automatic long-term retrieval in each turn. This short-term + long-term combination aligns with our architecture. Notably, the effectiveness of memory components depends on the agent’s ability to use them correctly. Recent studies compare various memory tools on benchmarks like LoCoMo. For example, Mem0 reported a 68.5% F1 on LoCoMo QA, while a simple file-based retrieval loop achieved 74.0%. This highlights an important fact: an agent’s skill in using tools often matters more than the tools themselves. Therefore, our system design pays special attention to prompt strategies and routing rules, ensuring the LLM fully leverages the information from multiple memory layers.

Cross-Lingual Retrieval and Knowledge Transfer

In cross-lingual contexts, prior work has focused on training multilingual LLMs or using translation. Off-the-shelf multilingual LLMs perform reasonably well on surface tasks like translation but still exhibit gaps in implicit cross-lingual reasoning. To enhance cross-lingual knowledge retrieval, common methods include: (1) Using multilingual embedding models that map different languages into a shared semantic vector space, enabling effective matching when the query and stored content are in different languages; (2) Using a pivot language strategy, e.g., translating a non-English query into English to search an English knowledge base, then translating the answer back. Our system adopts the former strategy: we index memory using language-agnostic embeddings so that a query in any language can retrieve relevant information stored in other languages. This approach greatly alleviates cross-lingual knowledge barriers and improves consistency in multilingual dialogues. In summary, our work builds on these foundations by integrating hierarchical memory (STM + LTM + UP), semantic retrieval, and multilingual processing, aiming to create a dialogue agent that can “remember” user information and conversation history across languages. The following sections describe our architecture, implementation, and evaluation.


III. System Architecture

The proposed system consists of three memory modules and a corresponding mechanism for prompt injection and model coordination, as illustrated in Figure 1. The main components are:
	•	Short-Term Memory (STM): Stores the latest important content of the current conversation, with a lifespan limited to the current session or the most recent few turns. Implemented as structured YAML, STM holds key points or summaries of recent interaction. Before each turn, STM is dynamically updated (e.g., by extracting new facts or clarifications from the previous turn’s user input). This ensures the model can quickly retrieve “what just happened.” Because STM capacity is limited, it typically retains only the most relevant information from the last few turns to avoid overly long or stale prompts.
	•	Long-Term Memory (LTM): Persists important information across sessions, including user-provided profiles, facts, events, etc. LTM is implemented as an SQLite database combined with vector indexing. Each memory item (e.g., a character’s backstory or a user’s favorite music genre) is stored as a database record, and its text is embedded with a multilingual embedding model to build a semantic index. LTM content survives across sessions, simulating long-term memory. When a user’s query involves past information, the system queries LTM to retrieve relevant memory chunks and injects them into the prompt. Compared to concatenating long histories directly, our LTM approach retrieves on-demand via semantic search, greatly reducing prompt length and irrelevant noise.
	•	User Profile (UP): Stores static user preferences, attributes, and personalization settings in a structured JSON document. UP can be seen as a special long-term memory but is more structured, explicitly containing fields like user name, age, language preference, professional background, and any explicitly stated preferences (e.g., preferring concise answers or formal address). At the start of each conversation, UP is extracted and injected into the prompt so the model knows who the user is and what they prefer, enabling personalized responses from the outset. UP is initially populated from user registration or long-term interaction, and can be updated over time (e.g., if the user repeatedly expresses dislike for a topic, the system updates that preference field).

These three layers work in a coordinated, hierarchical manner: STM handles short-term context, LTM covers long-term knowledge, and UP provides personalization. To make the LLM effectively utilize these memories, we design a prompt construction and routing mechanism. The core idea is: according to the current dialogue state and user request, we inject memory content from appropriate layers into the prompt at suitable positions, guiding the model to use these memories in its response without fine-tuning the model. The mechanism proceeds as follows:
	1.	Message Routing & Intent Recognition: Upon receiving user input, the agent’s routing module first classifies the message type and intent. For example, through rules or classifiers, it detects if the user query mentions past events, names, preference keywords, etc. If the query contains cues like “as I told you before…?” or “do you remember…?”, or if it involves historical facts, we decide to use long-term memory. If the request relates to personal preferences (e.g., “Recommend a song I might like”), we use the user profile. Otherwise, the agent focuses on STM and recent context.
	2.	Memory Retrieval: Based on the routing decision, the system retrieves relevant information from the corresponding memory stores. For historical questions, we encode the user’s query (or entities within it) into a vector and perform a semantic search in the LTM index, fetching the top relevant memories (such as past answers or factual snippets). For preference-related queries, we pull relevant fields from the UP (e.g., the user’s preferred music genre). Additionally, we always retrieve the latest summary from STM.
	3.	Prompt Construction and Injection: The prompt assembly module integrates the retrieved memory from all sources into the final prompt. We use a tiered injection strategy: first, we insert a summary of the user profile (e.g., in JSON format: “User Profile: {…}”); then in the next system message we list the long-term memory items (e.g., using a YAML or bullet list: “Memory: - Last year you said you like rock music; - Alice’s father is named John.”); next we inject the short-term memory summary (recent conversation key points in YAML); finally, we include the regular dialogue instructions and the user’s latest question. The resulting message sequence sent to the LLM has system messages containing the needed user information and memory context, followed by the conversation prompt. Importantly, we annotate each memory source to help the model distinguish them. For example, we prefix UP content with “[User Preference]” and LTM results with “[Historical Fact]”, using clear formatting (like Markdown lists or YAML blocks). This cues the model that these statements are recalled memory rather than user utterances, reducing hallucination.
	4.	Response Generation and Memory Update: After the LLM generates a response, the system parses the output and the current dialogue turn to extract any new key information, updating STM or LTM accordingly. For instance, if the user provides new personal information (e.g., “I got a new job at Google.”), we parse and store it in the LTM user info table for future retrieval. If a new agreement or story development occurs, we summarize and write it to STM to keep it in the recent context. Through this loop of updating memory, the system achieves continual learning: the more the interaction, the richer the long-term memory, and the better the agent’s understanding of the user and context.

In this collaborative mechanism, we define routing rules to balance performance and relevance. For example, each turn retrieves only a small number of LTM entries (e.g., top-3) to avoid excessively long prompts. For clear preference queries, we prioritize UP over LTM to directly use the user’s known preference. For cross-lingual queries, if no results are found in the current language, the system automatically translates the query into another language and searches again, maximizing recall of relevant memories. We also use language detection on user input to decide the reply language, ensuring natural multilingual dialogue. For example, if the user switches from English to Chinese, the system will automatically respond in Chinese and adjust LTM queries accordingly. These details ensure the system remains fluid and accurate in multi-language environments. In summary, our architecture fully leverages three memory types (STM, LTM, UP) with intelligent prompt injection and routing, enabling the LLM to participate in dialogue as if it had memory. Compared to simply using the model’s parameters or a very large context window, our approach is scalable, updatable, and session-continuous: new knowledge can be added by updating the database without retraining the model; different users’ memories are isolated (preventing privacy leakage); and sensitive data can be stored locally for safety.


IV. System Implementation

We have implemented a prototype of the described system with a modular design, encompassing components for memory storage, embedding retrieval, prompt assembly, and dialogue management. The backend is written in Python, leveraging open-source tools for efficiency. For example, we use a multilingual Sentence-BERT model to generate text embeddings and the Faiss library for fast approximate nearest-neighbor search to support LTM. STM and UP are stored as files (YAML and JSON, respectively) and updated via lightweight I/O. An SQLite database manages the long-term memory tables (including conversation logs, knowledge chunks, user info, etc.) and stores metadata for the vector index.

In LTM, we implement two core interfaces: add_memory and search_memory(query). The add_memory interface inserts new memory items (e.g., when a new fact is detected in the conversation), generates its embedding, and updates the index. The search_memory(query) interface encodes the input query into a vector and performs a nearest-neighbor search in the Faiss index to retrieve relevant memory candidates. We combine this with keyword filtering and timestamp metadata for fine-grained ranking (for example, favoring memories related to the same character). STM updates are triggered automatically by the dialogue manager: after each turn, it summarizes recent conversation points (up to a limit, e.g., 5 bullet points) and writes them to a YAML file. UP is initialized from user registration data and can be refined by observing user behavior (e.g., if the user repeatedly uses a certain language, record that as the preferred language).

For prompt assembly and templates, we use configurable templates to define the injection format. A YAML-based template outlines the general structure of the system prompt, with placeholders replaced at runtime. For instance, consider the following template snippet:

system_prompt: |
  You are an AI assistant. Please answer the user’s question using the following user profile and memory.
  User Profile:
  {{ user_profile_json }}
  Conversation History:
  {{ relevant_memories_list }}
  Current Context:
  {{ recent_summary }}

Here {{ user_profile_json }}, etc., are replaced by the content from UP, LTM, and STM. This templated approach allows flexible formatting without code changes. We experimented with plaintext, JSON, and YAML formats, finding that semistructured formatting (as above, with clearly labeled sections) works best. Explicitly labeling the source of information in the prompt helps the model distinguish memory content from user input, reducing confusion.

For multilingual support, our implementation ensures consistency across languages. UP fields are stored in multilingual form when possible (e.g., storing the user’s name in original script and in Pinyin or translation, and language preference as a code “zh”/“en”). During LTM retrieval, if a query is in Chinese but most memories are in English (or vice versa), the system automatically translates the query and performs retrieval in both languages, merging results to improve recall. We also use language detection on user input to determine the response language, ensuring the agent naturally follows language switches. For example, if the user switches from English to Chinese mid-session, the system recognizes this and responds in Chinese while also adjusting memory retrieval to the new language. These details help the system maintain fluency and accuracy in a multilingual environment.

Finally, we currently use the OpenAI GPT-4 API as the conversation model due to its strong bilingual capabilities and prompt sensitivity. The assembled messages (system and user messages) are sent via OpenAI’s chat interface, and responses are received. To reduce latency, we parallelize some operations (e.g., precomputing potential memory updates during the model’s thinking time). Because GPT-4 calls can be costly, we also experimented with an open-source multilingual model (such as MOSS or XGLM) to verify generality. Although such models lag GPT-4 in complex reasoning, with our memory-enhanced prompts they still produce reasonable answers in our scenarios. This demonstrates that our architecture does not depend on a specific LLM and its core ideas are model-agnostic.



V. Experiments and Evaluation

We conducted a series of experiments to evaluate the effectiveness of our architecture on character memory retention, preference adherence, and cross-turn consistency. The evaluation includes quantitative tests on benchmark dialogue data and qualitative case analyses. The setup is as follows:

A. Experimental Setup: For quantitative evaluation, we use a subset of the LoCoMo long-dialogue memory benchmark focusing on QA tasks. The LoCoMo dataset contains dialogues up to 300 turns and associated memory questions. We selected 50 QA pairs involving character relationships and events as our test set to measure the agent’s accuracy in recalling background facts. Additionally, we designed custom scenario scripts to test preference following and consistency. For example, one scenario has a user stating a language preference (e.g., “Please answer in Chinese”) at the beginning and then switching languages later to see if the agent respects that preference. Another scenario has the user mention personal information in one session (e.g., the name of a pet) and refer to it in the next session to test multi-session memory. We compare four system configurations (detailed in Table I): (1) No Memory: STM, LTM, and UP are all disabled (the agent relies only on the model’s built-in knowledge and the immediate recent context); (2) STM Only: short-term memory injection is enabled (recent summary is added), but no long-term retrieval or user profile; (3) LTM Only: long-term memory retrieval is enabled, but we do not inject the STM summary or UP; (4) Full Memory (STM+LTM+UP): the complete system with all three layers active. For each configuration, we record the following metrics: QA-Acc (accuracy of answers), Memory-F1 (F1 score comparing answer content to ground truth memory), Pref-F1 (F1 score for correctly following stated preferences), Hallucination Rate (Hall%) – the percentage of answers containing fabricated content not in memory or dialogue, Recall Correctness (RC%) – the percentage of memory-dependent queries answered correctly using the correct memory, and average latency per response.

Table I summarizes the experimental configurations.

Table I. Experimental Setup (demo)

Config	Host/Model	Version/Date (ex.)	Memory Injection	Vector Search	Environment	Avg Tokens	P95
No Memory	GPT-4o-mini	2025-08	None	None	Xeon/4090	350	1.7
STM Only	GPT-4o-mini	2025-08	YAML summary	None	Xeon/4090	650	1.9
LTM Only	GPT-4o	2025-08	LTM Top-k	FAISS	Xeon/4090	520	2.2
Full System	Multiple (Gemini/Claude/GPT-4o)	2025-09	YAML+LTM+UP	FAISS	Xeon/4090	780	2.4

B. Results: Table II summarizes the performance metrics for each configuration. The No Memory baseline performed the worst: QA accuracy was only 54%, Memory-F1 was 0.35, Pref-F1 was 0.38, hallucination rate was 18%, and RC% was 62%. In this case, the agent failed to recall earlier information and often answered incorrectly or not at all. With STM Only, accuracy improved to 66%, Memory-F1 to 0.48, Pref-F1 to 0.50, hallucination rate dropped to 14%, and RC% reached 74%. This shows that injecting recent summaries helped with short-term questions, reducing guessing errors, but the agent still missed long-past information. With LTM Only, performance increased further: QA accuracy reached 71%, Memory-F1 0.63, Pref-F1 0.66, hallucination rate 11%, and RC% 78%. The agent could retrieve past facts and answer many historical questions correctly (e.g., “What is Alice’s father’s name?”), significantly reducing hallucinations. Without STM, however, the agent sometimes lacked awareness of immediate context and missed preference cues (e.g., giving an answer in English despite a user preference for Chinese). The Full Memory (STM+LTM+UP) configuration achieved the best results on all metrics: QA accuracy 84% (about 30 percentage points above baseline), Memory-F1 0.79, Pref-F1 0.82, hallucination rate 6%, and RC% 88%. The combination of STM and LTM allowed the agent to handle both recent and long-term questions effectively. Including UP also improved preference adherence: in the preference scenario, the full system consistently honored the user’s language preference (baseline and STM-only agents sometimes ignored it and replied in English, while the full system always responded in the user’s preferred language). Overall, the multi-layer memory approach significantly improved QA accuracy and consistency while reducing hallucinations.

Table II. LoCoMo Subset and Scenario Results (demo) (↑ higher is better; Hall% lower is better)

Config	QA-Acc (Single Turn)	QA-Acc (Multi Turn)	Memory-F1	Pref-F1	Hall%	RC%	P95 (s)	Avg Tokens
No Memory	54	31	0.35	0.38	18	62	1.7	350
STM Only	66	38	0.48	0.50	14	74	1.9	650
LTM Only	71	52	0.63	0.66	11	78	2.2	520
Full System	84	68	0.79	0.82	6	88	2.4	780

For example, in the preference scenario, the No Memory agent typically failed to respect the user’s language preference. Our system’s Full Memory agent, by contrast, always replied in the preferred language because UP explicitly recorded “preferred_language: zh”. In the character memory scenario, when the user provided Alice’s backstory early on, only the LTM and Full agents could recall Alice’s details later. For instance, in turn 10 when asked “What is Alice’s father’s name?”, only the LTM-enabled systems answered correctly “John.” In consistency tests, when the user stated “I am 30 years old” in turn 1 and was asked again in turn 8, the No Memory agent gave an inconsistent age while the Full Memory agent correctly answered “30,” maintaining coherence. These cases illustrate the practical benefits of the multi-layer memory.

C. Performance Overhead: We also measured the runtime cost of our memory system. As shown in Table I, the average P95 response latency was roughly 1.7s for No Memory, 1.9s for STM, 2.2s for LTM, and 2.4s for Full Memory (all on GPT-4 with our setup). The Full system incurs about 0.5–0.7s extra delay due to vector search and longer prompts. This overhead is modest given the substantial gains in accuracy and consistency: sub-3s responses remain usable in interactive settings. In terms of prompt length, the average context size grew from 350 tokens (baseline) to 780 tokens (full memory). Although the prompt is longer, it is still far below the thousands of tokens that would be needed for the full raw history. In summary, our method achieves a significant improvement in dialogue quality and continuity at the cost of only moderate latency and prompt growth, a favorable trade-off.



VI. Future Work

Our multi-layer memory architecture for cross-lingual agents has shown promising initial results. For future work, we plan to explore the following improvements:
	•	Memory Retrieval Refinement: Currently, LTM retrieval relies on semantic similarity, which can sometimes return only loosely relevant content. We intend to incorporate structured approaches, such as knowledge graphs or topic clustering, to improve retrieval precision. We may also assign importance weights or tags to memories so the agent can distinguish crucial facts from less relevant details. Another direction is enabling active memory retrieval, allowing the model to dynamically query memory while generating responses to refine its answers further.
	•	Enhanced Cross-Lingual Memory: To further bridge cross-lingual gaps, we consider “bilingual memory” storage: key long-term memories would store both English and Chinese versions, or we would automatically translate retrieved memories into the user’s current language before injecting them. This way, the model does not have to self-translate, reducing errors. We also plan to fine-tune prompts on multilingual LLMs so they better understand our memory injection formats and cross-lingual context. For example, few-shot training could teach the model to rely on the injected memory rather than uncertain internal association.
	•	Lifelong Learning and Memory Decay: As the memory database grows over prolonged interactions, we plan to implement memory consolidation and pruning strategies. One approach is to periodically summarize old memories into high-level chunks and archive detailed entries, mimicking human memory consolidation. This prevents bloat from redundant or outdated memories. We also aim to allow the agent to learn from its experiences: right now the model only accesses memory at inference time, but in future we could fine-tune the model on collected memory data (with user consent), effectively internalizing frequently used knowledge. This hybrid memory approach would balance external storage with learned knowledge.
	•	Multimodal Memory: So far we have focused on text. A natural extension is to include multimodal content. Users might show images or speak in dialogue, and the agent should remember visual or auditory information. For example, if the user shares a photo of their pet, the agent could store an embedding or caption (e.g., “Image of a brown dog (Rex)”) in LTM. Later, when the user mentions “my dog,” the agent could retrieve that image memory and recall details. This would likely involve integrating vision models (like CLIP) to align image and text memory. Similar concepts apply to audio: the agent could store transcripts or audio features as memory. These extensions would move our agent toward a truly general assistant with memory across modalities.
	•	Deployment and Adaptation: Finally, we aim to deploy and adapt this system in real-world settings (e.g., a bilingual customer service chatbot). Deployment raises considerations such as privacy, performance, and integration. We will explore encrypted or client-side memory storage for sensitive data, and fine-grained access control so only authorized contexts can use certain memories. Performance optimizations like caching frequent queries or using faster embedding models will help maintain responsiveness. We also plan to incorporate user feedback loops, allowing users to correct the agent’s memories (e.g., “No, that’s wrong, I live in Paris, not London”), which will help refine memory accuracy. Extending the system to more languages (especially low-resource languages) may involve using or training appropriate multilingual models or translation pipelines.

In summary, future work will focus on scalability, robustness, and versatility of our memory-augmented agent. By refining retrieval, enhancing cross-lingual handling, enabling lifelong learning, and expanding to multiple modalities, we aim to push the boundaries of what AI assistants can remember and utilize.



VII. Conclusion

We have presented a system that integrates prompt engineering with a multi-layered memory architecture to enhance long-term and cross-lingual capabilities of dialogue agents. We analyzed the memory limitations faced by LLMs in extended and multilingual conversations and designed a system comprising Short-Term Memory for recent context, Long-Term Memory for persistent knowledge, and a User Profile for personalization. With intelligent prompt injection and routing, our agent effectively “remembers” key dialogue facts and user preferences, maintaining coherence across turns, sessions, and languages. Experiments confirm the architecture’s clear benefits: multi-layer memory substantially improved answer accuracy and dialogue consistency while significantly reducing hallucinations, at the cost of only modest additional computation. This demonstrates that architectural optimization – rather than simply scaling up model size – can more efficiently address the long-term memory problem in LLMs, offering a practical path to AI assistants with “lasting memory.”

This work is a mid-term demonstration with much room for improvement. In future work, we will continue to optimize memory retrieval, strengthen cross-lingual capabilities, and explore continual learning and multimodal memory, as discussed. We also plan to share our data and code with the community to advance research on memory mechanisms in AI agents. In summary, endowing dialogue agents with persistent, reliable memory is a crucial step toward building smarter, more empathetic AI. We believe that our multi-layered memory and prompt design approach moves us closer to an AI agent that deeply understands and adapts to users over long-term, multilingual interactions.

References:
	1.	【24】L. P. Tran et al., “Benchmarking AI Agent Memory: Is a Filesystem All You Need?,” Letta Blog, Aug. 2025.
	2.	【16】A. Maharana et al., “Evaluating Very Long-Term Conversational Memory of LLM Agents,” arXiv preprint arXiv:2402.17753, 2024.
	3.	【21】L. Chua et al., “Crosslingual Capabilities and Knowledge Barriers in Multilingual LLMs,” arXiv preprint arXiv:2406.16135, 2024.
	4.	【8】A. Maharana et al., “LoCoMo: A Dataset for Long-Context Memory in Dialogues,” Snap Research, 2024.
	5.	【14】Letta Research, “Benchmarking Memory Tools with LoCoMo (Mem0, LangMem, Zep),” Blog post, 2025.
	6.	【15】C. Packer et al., “MemGPT: Towards LLMs as Operating Systems,” arXiv preprint arXiv:2310.08560, 2023.
	7.	【39】L. Guan, “MemGPT Series Project Analysis: Open-Source Memory-Enhanced Agent mem0,” CnBlogs Tech Blog, 2024 (in Chinese).
	8.	【12】Gibson AI, “Memori: Open-Source Memory Engine for LLMs,” GitHub Repository, 2023.
	9.	【20】A. Masood, “Optimizing Chunking, Embedding, and Vectorization for RAG,” Medium, Jul. 2023.
	10.	【31】Snap Research, “Findings – LoCoMo Benchmark,” Project Page, 2024.
	11.	【25】mem0 Team, “Mem0 Research Highlights,” GitHub, 2023.  
	12.	【23】C. Le, “Agentforce Multilingual Support: Anatomy of a Global Agent,” Medium, Nov. 2024. 