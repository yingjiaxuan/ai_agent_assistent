# -*- coding: utf-8 -*-
"""
Prompt Experiment Results Analysis Script
========================================
åˆ†æPrompt Engineeringå®éªŒç»“æœï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œç»Ÿè®¡æŠ¥å‘Š

åŠŸèƒ½ï¼š
1. è¯»å–å®éªŒç»“æœCSVæ–‡ä»¶
2. è®¡ç®—å„ç§Promptè®¾è®¡çš„æˆåŠŸç‡ç»Ÿè®¡
3. ç”Ÿæˆå¯¹æ¯”å›¾è¡¨ï¼ˆå¦‚æœå®‰è£…äº†matplotlibï¼‰
4. è¾“å‡ºè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
python tests/analyze_experiment_results.py
"""

import csv
import json
from pathlib import Path
from collections import defaultdict

def load_experiment_results(csv_path: Path) -> list:
    """
    åŠ è½½å®éªŒç»“æœCSVæ–‡ä»¶
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        list: å®éªŒç»“æœæ•°æ®åˆ—è¡¨
    """
    if not csv_path.exists():
        print(f"âŒ Results file not found: {csv_path}")
        return []
    
    results = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # è½¬æ¢æ•°å€¼å­—æ®µ
            row['temperature'] = float(row['temperature'])
            row['top_p'] = float(row['top_p'])
            row['json_parseable'] = row['json_parseable'] == 'True'
            row['required_fields_present'] = int(row['required_fields_present'])
            row['field_completeness_rate'] = float(row['field_completeness_rate'])
            row['response_length'] = int(row['response_length'])
            row['empty_fields_count'] = int(row['empty_fields_count'])
            results.append(row)
    
    return results

def analyze_by_prompt_type(results: list) -> dict:
    """
    æŒ‰Promptç±»å‹åˆ†æç»“æœ
    
    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        
    Returns:
        dict: æŒ‰promptç±»å‹æ±‡æ€»çš„ç»Ÿè®¡æ•°æ®
    """
    stats = defaultdict(lambda: {
        'total_runs': 0,
        'successful_parses': 0,
        'avg_completeness': 0.0,
        'avg_response_length': 0.0,
        'parse_success_rate': 0.0
    })
    
    for result in results:
        prompt_type = result['prompt_type']
        stats[prompt_type]['total_runs'] += 1
        
        if result['json_parseable']:
            stats[prompt_type]['successful_parses'] += 1
        
        stats[prompt_type]['avg_completeness'] += result['field_completeness_rate']
        stats[prompt_type]['avg_response_length'] += result['response_length']
    
    # è®¡ç®—å¹³å‡å€¼
    for prompt_type in stats:
        total = stats[prompt_type]['total_runs']
        if total > 0:
            stats[prompt_type]['parse_success_rate'] = stats[prompt_type]['successful_parses'] / total
            stats[prompt_type]['avg_completeness'] /= total
            stats[prompt_type]['avg_response_length'] /= total
    
    return dict(stats)

def analyze_by_parameters(results: list) -> dict:
    """
    æŒ‰APIå‚æ•°åˆ†æç»“æœ
    
    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        
    Returns:
        dict: æŒ‰å‚æ•°ç»„åˆæ±‡æ€»çš„ç»Ÿè®¡æ•°æ®
    """
    param_stats = defaultdict(lambda: {
        'total_runs': 0,
        'successful_parses': 0,
        'avg_completeness': 0.0
    })
    
    for result in results:
        key = f"temp_{result['temperature']}_top_p_{result['top_p']}"
        param_stats[key]['total_runs'] += 1
        
        if result['json_parseable']:
            param_stats[key]['successful_parses'] += 1
        
        param_stats[key]['avg_completeness'] += result['field_completeness_rate']
    
    # è®¡ç®—å¹³å‡å€¼å’ŒæˆåŠŸç‡
    for key in param_stats:
        total = param_stats[key]['total_runs']
        if total > 0:
            param_stats[key]['parse_success_rate'] = param_stats[key]['successful_parses'] / total
            param_stats[key]['avg_completeness'] /= total
    
    return dict(param_stats)

def generate_report(results: list) -> str:
    """
    ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
    
    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
    """
    if not results:
        return "âŒ No experiment data available for analysis."
    
    prompt_stats = analyze_by_prompt_type(results)
    param_stats = analyze_by_parameters(results)
    
    report = []
    report.append("ğŸ”¬ Prompt Engineering Experiment Analysis Report")
    report.append("=" * 60)
    report.append(f"ğŸ“Š Total experiments conducted: {len(results)}")
    
    # æŒ‰Promptç±»å‹çš„åˆ†æ
    report.append("\nğŸ“‹ Analysis by Prompt Type:")
    report.append("-" * 40)
    
    for prompt_type, stats in prompt_stats.items():
        report.append(f"\nğŸ”¹ {prompt_type.upper()}:")
        report.append(f"   â€¢ Success Rate: {stats['parse_success_rate']:.1%}")
        report.append(f"   â€¢ Avg Completeness: {stats['avg_completeness']:.1%}")
        report.append(f"   â€¢ Avg Response Length: {stats['avg_response_length']:.0f} chars")
        report.append(f"   â€¢ Total Runs: {stats['total_runs']}")
    
    # æœ€ä½³Promptç±»å‹
    best_prompt = max(prompt_stats.keys(), 
                     key=lambda x: prompt_stats[x]['parse_success_rate'])
    report.append(f"\nğŸ† Best Performing Prompt: {best_prompt}")
    report.append(f"   Success Rate: {prompt_stats[best_prompt]['parse_success_rate']:.1%}")
    
    # æŒ‰å‚æ•°çš„åˆ†æ
    report.append(f"\nğŸ›ï¸  Analysis by API Parameters:")
    report.append("-" * 40)
    
    for param_combo, stats in sorted(param_stats.items()):
        report.append(f"\nğŸ”¹ {param_combo}:")
        report.append(f"   â€¢ Success Rate: {stats['parse_success_rate']:.1%}")
        report.append(f"   â€¢ Avg Completeness: {stats['avg_completeness']:.1%}")
    
    # æœ€ä½³å‚æ•°ç»„åˆ
    best_params = max(param_stats.keys(), 
                     key=lambda x: param_stats[x]['parse_success_rate'])
    report.append(f"\nğŸ† Best Parameter Combination: {best_params}")
    report.append(f"   Success Rate: {param_stats[best_params]['parse_success_rate']:.1%}")
    
    # æ€»ç»“å’Œå»ºè®®
    report.append(f"\nğŸ’¡ Key Insights:")
    report.append("-" * 20)
    
    # è®¡ç®—æ€»ä½“æˆåŠŸç‡
    total_success = sum(1 for r in results if r['json_parseable'])
    overall_success_rate = total_success / len(results)
    report.append(f"â€¢ Overall JSON parsing success rate: {overall_success_rate:.1%}")
    
    # Promptç±»å‹å¯¹æ¯”
    prompt_success_rates = {pt: stats['parse_success_rate'] 
                           for pt, stats in prompt_stats.items()}
    sorted_prompts = sorted(prompt_success_rates.items(), 
                           key=lambda x: x[1], reverse=True)
    
    report.append(f"â€¢ Prompt effectiveness ranking:")
    for i, (prompt_type, rate) in enumerate(sorted_prompts, 1):
        report.append(f"  {i}. {prompt_type}: {rate:.1%}")
    
    # å‚æ•°å½±å“åˆ†æ
    temp_analysis = defaultdict(list)
    for result in results:
        temp_analysis[result['temperature']].append(result['json_parseable'])
    
    report.append(f"\nâ€¢ Temperature impact:")
    for temp in sorted(temp_analysis.keys()):
        success_rate = sum(temp_analysis[temp]) / len(temp_analysis[temp])
        report.append(f"  temp={temp}: {success_rate:.1%} success rate")
    
    return "\n".join(report)

def save_analysis_report(report: str, output_path: Path):
    """
    ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
    
    Args:
        report: æŠ¥å‘Šæ–‡æœ¬
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“„ Analysis report saved to: {output_path}")

def main():
    """
    ä¸»åˆ†ææµç¨‹
    """
    print("ğŸ“ˆ Starting experiment results analysis...")
    
    # å®šä½ç»“æœæ–‡ä»¶
    results_dir = Path(__file__).parent / "experiment_results"
    csv_file = results_dir / "prompt_comparison_results.csv"
    
    if not csv_file.exists():
        print(f"âŒ No experiment results found at: {csv_file}")
        print("ğŸ’¡ Please run the experiment first: python tests/test_prompt_experiments.py")
        return
    
    # åŠ è½½å’Œåˆ†ææ•°æ®
    results = load_experiment_results(csv_file)
    if not results:
        return
    
    print(f"âœ… Loaded {len(results)} experiment records")
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report = generate_report(results)
    print("\n" + report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = results_dir / "analysis_report.txt"
    save_analysis_report(report, report_file)
    
    # å°è¯•ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆå¯é€‰ï¼‰
    try:
        import matplotlib.pyplot as plt
        import pandas as pd
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºå¯è§†åŒ–
        df = pd.DataFrame(results)
        
        # åˆ›å»ºæˆåŠŸç‡å¯¹æ¯”å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. Promptç±»å‹æˆåŠŸç‡å¯¹æ¯”
        prompt_success = df.groupby('prompt_type')['json_parseable'].mean()
        prompt_success.plot(kind='bar', ax=ax1, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax1.set_title('JSON Parsing Success Rate by Prompt Type')
        ax1.set_ylabel('Success Rate')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. æ¸©åº¦å‚æ•°å½±å“
        temp_success = df.groupby('temperature')['json_parseable'].mean()
        temp_success.plot(kind='line', marker='o', ax=ax2, color='#FF6B6B')
        ax2.set_title('Success Rate vs Temperature')
        ax2.set_xlabel('Temperature')
        ax2.set_ylabel('Success Rate')
        
        # 3. å­—æ®µå®Œæ•´æ€§å¯¹æ¯”
        completeness = df.groupby('prompt_type')['field_completeness_rate'].mean()
        completeness.plot(kind='bar', ax=ax3, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax3.set_title('Field Completeness Rate by Prompt Type')
        ax3.set_ylabel('Completeness Rate')
        ax3.tick_params(axis='x', rotation=45)
        
        # 4. å“åº”é•¿åº¦åˆ†å¸ƒ
        df.boxplot(column='response_length', by='prompt_type', ax=ax4)
        ax4.set_title('Response Length Distribution by Prompt Type')
        ax4.set_ylabel('Response Length (chars)')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = results_dir / "analysis_charts.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Visualization charts saved to: {chart_file}")
        
    except ImportError:
        print("ğŸ“Š Matplotlib not available - skipping visualization")
        print("ğŸ’¡ Install with: pip install matplotlib pandas")

if __name__ == "__main__":
    main() 