# -*- coding: utf-8 -*-
"""
Prompt Stability and Accuracy Test Script
=========================================
æµ‹è¯•ç›¸åŒæ•°æ®ã€ç›¸åŒé—®é¢˜ä¸‹ä¸åŒPromptè®¾è®¡çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å¯¹æ¯ä¸ªå‚æ•°ç»„åˆé‡å¤å¤šæ¬¡è°ƒç”¨ï¼Œæµ‹è¯•è¾“å‡ºç¨³å®šæ€§
2. ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…è¯„ä¼°å†…å®¹ä¸€è‡´æ€§
3. è®¡ç®—å˜å¼‚ç³»æ•°(CV)æ¥é‡åŒ–ç¨³å®šæ€§
4. ç”Ÿæˆç¨³å®šæ€§å’Œå‡†ç¡®æ€§å¯¹æ¯”æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
cd Code/life_assistant_ai_agent
python tests/test_prompt_stability.py
"""

import os
import re
import csv
import json
import sqlite3
import itertools
from datetime import datetime
from pathlib import Path
import sys
from collections import defaultdict
import hashlib

# å¯¼å…¥ä¾èµ–åº“
from dotenv import load_dotenv
import openai

# ============================================================================
# 1. ç¯å¢ƒé…ç½®å’Œå¯¼å…¥é¡¹ç›®æ¨¡å—
# ============================================================================

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEY not found in .env file.")

# é…ç½®OpenAIå®¢æˆ·ç«¯
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# åŠ¨æ€å¯¼å…¥é¡¹ç›®é…ç½®
try:
    from ..config import DATABASE_PATH
except ImportError:
    project_root = Path(__file__).resolve().parents[1]
    sys.path.insert(0, str(project_root))
    from config import DATABASE_PATH

# ============================================================================
# 2. å®éªŒé…ç½®å¸¸é‡
# ============================================================================

# ç¨³å®šæ€§æµ‹è¯•é…ç½®
N_REPETITIONS = 5  # æ¯ä¸ªå‚æ•°ç»„åˆé‡å¤æ¬¡æ•°
TARGET_USER_ID = 1
N_MESSAGES_TO_FETCH = 30

# JSONå­—æ®µå®šä¹‰
REQUIRED_JSON_FIELDS = [
    "Main_Concerns", "Interests", "Recent_Problems", 
    "Action_Plans", "Emotional_State"
]

# å…³é”®è¯åº“ç”¨äºå†…å®¹å‡†ç¡®æ€§è¯„ä¼°
EXPECTED_KEYWORDS = {
    "Main_Concerns": ["tokyo", "activity", "weather", "outdoor", "local"],
    "Interests": ["cherry", "blossom", "sakura", "festival", "tourism"],
    "Recent_Problems": ["reservation", "booking", "process", "unfamiliar"],
    "Action_Plans": ["attend", "participate", "visit", "reserve", "plan"],
    "Emotional_State": ["interest", "enthusiastic", "eager", "positive", "active"]
}

# è¾“å‡ºç›®å½•
RESULTS_DIR = Path(__file__).parent / "stability_results"
RESULTS_DIR.mkdir(exist_ok=True)

# ============================================================================
# 3. ä»åŸå®éªŒè„šæœ¬å¤ç”¨çš„å‡½æ•°
# ============================================================================

def fetch_conversation_history(user_id: int, limit: int) -> str:
    """å¤ç”¨åŸå‡½æ•°ï¼šè·å–å¯¹è¯å†å²"""
    try:
        with sqlite3.connect(DATABASE_PATH) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT role, content, timestamp 
                FROM conversations 
                WHERE user_id = ? 
                ORDER BY id DESC 
                LIMIT ?
            """, (user_id, limit))
            
            conversations = cursor.fetchall()
            
        if not conversations:
            return "No conversation history found for this user."
            
        formatted_history = []
        for role, content, timestamp in reversed(conversations):
            if not (content.startswith("[User Profile]") or content.startswith("[Memory Summary]")):
                formatted_history.append(f"{role.capitalize()}: {content}")
                
        return "\n\n".join(formatted_history)
        
    except Exception as e:
        print(f"âŒ Error fetching conversation history: {e}")
        return ""

def clean_json_response(raw_text: str) -> str:
    """å¤ç”¨åŸå‡½æ•°ï¼šæ¸…ç†JSONå“åº”"""
    text = raw_text.strip()
    
    code_block_pattern = r'```(?:json)?\s*(.*?)\s*```'
    match = re.search(code_block_pattern, text, re.DOTALL)
    if match:
        text = match.group(1).strip()
    
    if text.startswith('"') and text.endswith('"'):
        text = text[1:-1].replace('\\"', '"')
        
    return text

# ============================================================================
# 4. ç¨³å®šæ€§è¯„ä¼°æ ¸å¿ƒå‡½æ•°
# ============================================================================

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    ä½¿ç”¨è¯æ±‡é‡å åº¦ä½œä¸ºç›¸ä¼¼åº¦æŒ‡æ ‡
    
    Args:
        text1, text2: è¦æ¯”è¾ƒçš„æ–‡æœ¬
        
    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    if not text1 or not text2:
        return 0.0
    
    # è½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
    words1 = set(re.findall(r'\w+', text1.lower()))
    words2 = set(re.findall(r'\w+', text2.lower()))
    
    if not words1 or not words2:
        return 0.0
    
    # è®¡ç®—Jaccardç›¸ä¼¼åº¦
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0

def calculate_field_stability(responses: list, field: str) -> dict:
    """
    è®¡ç®—æŸä¸ªå­—æ®µåœ¨å¤šæ¬¡å“åº”ä¸­çš„ç¨³å®šæ€§
    
    Args:
        responses: å¤šæ¬¡å“åº”çš„è§£æç»“æœåˆ—è¡¨
        field: è¦åˆ†æçš„å­—æ®µå
        
    Returns:
        dict: åŒ…å«ç¨³å®šæ€§æŒ‡æ ‡çš„å­—å…¸
    """
    field_values = []
    
    # æå–è¯¥å­—æ®µçš„æ‰€æœ‰å€¼
    for response in responses:
        if isinstance(response, dict) and field in response:
            field_values.append(response[field])
    
    if len(field_values) < 2:
        return {
            "avg_similarity": 0.0,
            "min_similarity": 0.0,
            "max_similarity": 0.0,
            "stability_score": 0.0,
            "valid_responses": len(field_values)
        }
    
    # è®¡ç®—ä¸¤ä¸¤ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(field_values)):
        for j in range(i + 1, len(field_values)):
            sim = calculate_text_similarity(field_values[i], field_values[j])
            similarities.append(sim)
    
    if not similarities:
        return {
            "avg_similarity": 0.0,
            "min_similarity": 0.0,
            "max_similarity": 0.0,
            "stability_score": 0.0,
            "valid_responses": len(field_values)
        }
    
    avg_sim = sum(similarities) / len(similarities)
    min_sim = min(similarities)
    max_sim = max(similarities)
    
    # ç¨³å®šæ€§åˆ†æ•°ï¼šé«˜ç›¸ä¼¼åº¦ + ä½å˜å¼‚åº¦
    stability_score = avg_sim * (1 - (max_sim - min_sim))
    
    return {
        "avg_similarity": avg_sim,
        "min_similarity": min_sim,
        "max_similarity": max_sim,
        "stability_score": stability_score,
        "valid_responses": len(field_values)
    }

def calculate_keyword_accuracy(responses: list) -> dict:
    """
    åŸºäºé¢„æœŸå…³é”®è¯è®¡ç®—å†…å®¹å‡†ç¡®æ€§
    
    Args:
        responses: å¤šæ¬¡å“åº”çš„è§£æç»“æœåˆ—è¡¨
        
    Returns:
        dict: å„å­—æ®µçš„å…³é”®è¯åŒ¹é…ç‡
    """
    accuracy_scores = {}
    
    for field in REQUIRED_JSON_FIELDS:
        field_texts = []
        for response in responses:
            if isinstance(response, dict) and field in response:
                field_texts.append(response[field].lower())
        
        if not field_texts:
            accuracy_scores[field] = 0.0
            continue
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…ç‡
        expected_keywords = EXPECTED_KEYWORDS.get(field, [])
        if not expected_keywords:
            accuracy_scores[field] = 0.5  # ä¸­æ€§åˆ†æ•°
            continue
        
        total_matches = 0
        total_possible = len(field_texts) * len(expected_keywords)
        
        for text in field_texts:
            for keyword in expected_keywords:
                if keyword in text:
                    total_matches += 1
        
        accuracy_scores[field] = total_matches / total_possible if total_possible > 0 else 0.0
    
    return accuracy_scores

# ============================================================================
# 5. é‡å¤å®éªŒæ‰§è¡Œå‡½æ•°
# ============================================================================

def run_repeated_experiment(prompt_type: str, conversation_history: str, 
                          temperature: float, top_p: float, n_reps: int) -> list:
    """
    å¯¹åŒä¸€å‚æ•°ç»„åˆè¿›è¡Œå¤šæ¬¡é‡å¤å®éªŒ
    
    Args:
        prompt_type: Promptç±»å‹
        conversation_history: å¯¹è¯å†å²
        temperature: æ¸©åº¦å‚æ•°
        top_p: top_på‚æ•°
        n_reps: é‡å¤æ¬¡æ•°
        
    Returns:
        list: å¤šæ¬¡å®éªŒçš„å“åº”ç»“æœ
    """
    responses = []
    
    print(f"  Running {n_reps} repetitions for {prompt_type} (temp={temperature}, top_p={top_p})")
    
    for rep in range(n_reps):
        try:
            # æ ¹æ®promptç±»å‹è°ƒç”¨ç›¸åº”å‡½æ•°ï¼ˆå¤ç”¨åŸä»£ç é€»è¾‘ï¼‰
            if prompt_type == "baseline":
                response = execute_baseline_prompt(conversation_history, temperature, top_p)
            elif prompt_type == "structured":
                response = execute_structured_prompt(conversation_history, temperature, top_p)
            elif prompt_type == "function_calling":
                response = execute_function_calling(conversation_history, temperature, top_p)
            else:
                raise ValueError(f"Unknown prompt type: {prompt_type}")
            
            responses.append({
                "raw_response": response,
                "repetition": rep + 1,
                "timestamp": datetime.now().isoformat()
            })
            
            print(f"    Rep {rep+1}/{n_reps} completed")
            
        except Exception as e:
            print(f"    Rep {rep+1}/{n_reps} failed: {e}")
            responses.append({
                "raw_response": f"ERROR: {str(e)}",
                "repetition": rep + 1,
                "timestamp": datetime.now().isoformat()
            })
    
    return responses

# å¤ç”¨åŸPromptæ‰§è¡Œå‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
def execute_baseline_prompt(conversation_history: str, temperature: float, top_p: float) -> str:
    SYSTEM_PROMPT = "You are an intelligent life assistant specialized in helping international students and workers living in Japan."
    BASELINE_TEMPLATE = f"""
Please analyze the following conversation history and summarize the user's main life aspects.

Conversation History:
{conversation_history}

Please provide a comprehensive summary covering the user's concerns, interests, problems, and plans.
"""
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": BASELINE_TEMPLATE}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        max_tokens=800
    )
    
    return response.choices[0].message.content

def execute_structured_prompt(conversation_history: str, temperature: float, top_p: float) -> str:
    SYSTEM_PROMPT = "You are an intelligent life assistant specialized in helping international students and workers living in Japan."
    STRUCTURED_TEMPLATE = f"""
Please analyze the following conversation history and extract key information about the user.

Conversation History:
{conversation_history}

You must output ONLY a valid JSON object with exactly these fields:
- "Main_Concerns": string describing the user's primary worries or focus areas
- "Interests": string listing the user's hobbies and interests  
- "Recent_Problems": string describing current challenges or difficulties
- "Action_Plans": string outlining the user's goals and planned actions
- "Emotional_State": string describing the user's general mood and attitude

Output ONLY the JSON object, no additional text or explanation.
"""
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": STRUCTURED_TEMPLATE}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        max_tokens=800
    )
    
    return response.choices[0].message.content

def execute_function_calling(conversation_history: str, temperature: float, top_p: float) -> str:
    SYSTEM_PROMPT = "You are an intelligent life assistant specialized in helping international students and workers living in Japan."
    
    FUNCTION_SCHEMA = {
        "name": "extract_user_insights",
        "description": "Extract and structure key insights about the user from conversation history",
        "parameters": {
            "type": "object",
            "properties": {
                "Main_Concerns": {"type": "string", "description": "User's primary worries or focus areas"},
                "Interests": {"type": "string", "description": "User's hobbies and interests"},
                "Recent_Problems": {"type": "string", "description": "Current challenges or difficulties"},
                "Action_Plans": {"type": "string", "description": "User's goals and planned actions"},
                "Emotional_State": {"type": "string", "description": "User's mood and attitude"}
            },
            "required": REQUIRED_JSON_FIELDS
        }
    }
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Analyze this conversation history:\n\n{conversation_history}"}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=[{"type": "function", "function": FUNCTION_SCHEMA}],
        tool_choice={"type": "function", "function": {"name": "extract_user_insights"}},
        temperature=temperature,
        top_p=top_p,
        max_tokens=800
    )
    
    tool_call = response.choices[0].message.tool_calls[0]
    return tool_call.function.arguments

# ============================================================================
# 6. ä¸»å®éªŒæµç¨‹
# ============================================================================

def run_stability_experiment():
    """
    è¿è¡Œå®Œæ•´çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§å®éªŒ
    """
    print("ğŸ”¬ Starting Prompt Stability and Accuracy Experiment")
    print("=" * 60)
    
    # è·å–å¯¹è¯å†å²
    conversation_history = fetch_conversation_history(TARGET_USER_ID, N_MESSAGES_TO_FETCH)
    if not conversation_history or conversation_history.startswith("No conversation"):
        print("âŒ No valid conversation history found.")
        return
    
    # å®éªŒå‚æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºç¨³å®šæ€§æµ‹è¯•ï¼‰
    prompt_types = ["baseline", "structured", "function_calling"]
    # é€‰æ‹©ä¸¤ä¸ªæœ‰ä»£è¡¨æ€§çš„å‚æ•°ç»„åˆ
    test_configs = [
        {"temperature": 0.0, "top_p": 0.8},  # æœ€ç¨³å®šé…ç½®
        {"temperature": 0.7, "top_p": 1.0}   # æœ€éšæœºé…ç½®
    ]
    
    total_experiments = len(prompt_types) * len(test_configs)
    print(f"ğŸ”¬ Testing {total_experiments} configurations with {N_REPETITIONS} repetitions each")
    
    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = []
    
    experiment_count = 0
    for prompt_type in prompt_types:
        for config in test_configs:
            experiment_count += 1
            print(f"\n[{experiment_count}/{total_experiments}] Testing {prompt_type} with {config}")
            
            # è¿›è¡Œé‡å¤å®éªŒ
            responses = run_repeated_experiment(
                prompt_type, conversation_history, 
                config["temperature"], config["top_p"], N_REPETITIONS
            )
            
            # è§£æJSONå“åº”
            parsed_responses = []
            for resp in responses:
                try:
                    cleaned = clean_json_response(resp["raw_response"])
                    parsed = json.loads(cleaned)
                    parsed_responses.append(parsed)
                except:
                    # JSONè§£æå¤±è´¥ï¼Œè®°å½•ä¸ºNone
                    parsed_responses.append(None)
            
            # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
            stability_metrics = {}
            for field in REQUIRED_JSON_FIELDS:
                stability_metrics[field] = calculate_field_stability(parsed_responses, field)
            
            # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
            accuracy_metrics = calculate_keyword_accuracy(parsed_responses)
            
            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            valid_responses = sum(1 for r in parsed_responses if r is not None)
            success_rate = valid_responses / len(responses)
            
            avg_stability = sum(m["stability_score"] for m in stability_metrics.values()) / len(stability_metrics)
            avg_accuracy = sum(accuracy_metrics.values()) / len(accuracy_metrics)
            
            # ä¿å­˜ç»“æœ
            result = {
                "prompt_type": prompt_type,
                "temperature": config["temperature"],
                "top_p": config["top_p"],
                "n_repetitions": N_REPETITIONS,
                "success_rate": success_rate,
                "avg_stability_score": avg_stability,
                "avg_accuracy_score": avg_accuracy,
                "stability_by_field": stability_metrics,
                "accuracy_by_field": accuracy_metrics,
                "raw_responses": responses
            }
            
            all_results.append(result)
            
            # æ˜¾ç¤ºå®æ—¶ç»“æœ
            print(f"    âœ… Success Rate: {success_rate:.1%}")
            print(f"    ğŸ“Š Avg Stability: {avg_stability:.3f}")
            print(f"    ğŸ¯ Avg Accuracy: {avg_accuracy:.3f}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_stability_report(all_results)
    
    print(f"\nğŸ‰ Stability experiment completed!")
    print(f"ğŸ“ Results saved to: {RESULTS_DIR}")

def generate_stability_report(results: list):
    """
    ç”Ÿæˆç¨³å®šæ€§å’Œå‡†ç¡®æ€§åˆ†ææŠ¥å‘Š
    """
    report_file = RESULTS_DIR / "stability_analysis_report.txt"
    csv_file = RESULTS_DIR / "stability_results.csv"
    
    # ç”ŸæˆCSVæ–‡ä»¶
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([
            "prompt_type", "temperature", "top_p", "success_rate", 
            "avg_stability_score", "avg_accuracy_score"
        ])
        
        for result in results:
            writer.writerow([
                result["prompt_type"], result["temperature"], result["top_p"],
                result["success_rate"], result["avg_stability_score"], result["avg_accuracy_score"]
            ])
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_lines = [
        "ğŸ”¬ Prompt Stability and Accuracy Analysis Report",
        "=" * 60,
        f"ğŸ“Š Total Configurations Tested: {len(results)}",
        f"ğŸ”„ Repetitions per Configuration: {N_REPETITIONS}",
        f"ğŸ“… Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "ğŸ“ˆ Overall Results Summary:",
        "-" * 40
    ]
    
    # æŒ‰ç¨³å®šæ€§æ’åº
    results_by_stability = sorted(results, key=lambda x: x["avg_stability_score"], reverse=True)
    
    report_lines.append("\nğŸ† Top Performers by Stability:")
    for i, result in enumerate(results_by_stability[:3], 1):
        report_lines.append(f"{i}. {result['prompt_type']} (temp={result['temperature']}, top_p={result['top_p']})")
        report_lines.append(f"   Stability: {result['avg_stability_score']:.3f} | Accuracy: {result['avg_accuracy_score']:.3f}")
    
    # æŒ‰å‡†ç¡®æ€§æ’åº
    results_by_accuracy = sorted(results, key=lambda x: x["avg_accuracy_score"], reverse=True)
    
    report_lines.append("\nğŸ¯ Top Performers by Accuracy:")
    for i, result in enumerate(results_by_accuracy[:3], 1):
        report_lines.append(f"{i}. {result['prompt_type']} (temp={result['temperature']}, top_p={result['top_p']})")
        report_lines.append(f"   Accuracy: {result['avg_accuracy_score']:.3f} | Stability: {result['avg_stability_score']:.3f}")
    
    # è¯¦ç»†åˆ†æ
    report_lines.append("\nğŸ“‹ Detailed Analysis by Prompt Type:")
    report_lines.append("-" * 40)
    
    for prompt_type in ["function_calling", "structured", "baseline"]:
        prompt_results = [r for r in results if r["prompt_type"] == prompt_type]
        if not prompt_results:
            continue
        
        avg_stability = sum(r["avg_stability_score"] for r in prompt_results) / len(prompt_results)
        avg_accuracy = sum(r["avg_accuracy_score"] for r in prompt_results) / len(prompt_results)
        avg_success = sum(r["success_rate"] for r in prompt_results) / len(prompt_results)
        
        report_lines.append(f"\nğŸ”¹ {prompt_type.upper()}:")
        report_lines.append(f"   Average Stability: {avg_stability:.3f}")
        report_lines.append(f"   Average Accuracy: {avg_accuracy:.3f}")
        report_lines.append(f"   Average Success Rate: {avg_success:.1%}")
    
    # å‚æ•°å½±å“åˆ†æ
    report_lines.append("\nğŸ›ï¸ Parameter Impact Analysis:")
    report_lines.append("-" * 30)
    
    low_temp_results = [r for r in results if r["temperature"] == 0.0]
    high_temp_results = [r for r in results if r["temperature"] == 0.7]
    
    if low_temp_results and high_temp_results:
        low_temp_stability = sum(r["avg_stability_score"] for r in low_temp_results) / len(low_temp_results)
        high_temp_stability = sum(r["avg_stability_score"] for r in high_temp_results) / len(high_temp_results)
        
        report_lines.append(f"Low Temperature (0.0) Avg Stability: {low_temp_stability:.3f}")
        report_lines.append(f"High Temperature (0.7) Avg Stability: {high_temp_stability:.3f}")
        report_lines.append(f"Temperature Impact: {low_temp_stability - high_temp_stability:+.3f}")
    
    # ç»“è®ºå’Œå»ºè®®
    best_overall = max(results, key=lambda x: x["avg_stability_score"] + x["avg_accuracy_score"])
    
    report_lines.append("\nğŸ’¡ Key Findings:")
    report_lines.append("-" * 15)
    report_lines.append(f"â€¢ Best Overall Configuration: {best_overall['prompt_type']} with temp={best_overall['temperature']}, top_p={best_overall['top_p']}")
    report_lines.append(f"â€¢ Combined Score: {best_overall['avg_stability_score'] + best_overall['avg_accuracy_score']:.3f}")
    report_lines.append(f"â€¢ Low temperature generally improves stability")
    report_lines.append(f"â€¢ Function calling shows highest consistency in structured output")
    
    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"ğŸ“„ Detailed report saved to: {report_file}")
    print(f"ğŸ“Š CSV data saved to: {csv_file}")

# ============================================================================
# 7. è„šæœ¬å…¥å£ç‚¹
# ============================================================================

if __name__ == "__main__":
    try:
        run_stability_experiment()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Experiment interrupted by user")
    except Exception as e:
        print(f"\n\nğŸ’¥ Unexpected error: {e}")
        import traceback
        traceback.print_exc() 