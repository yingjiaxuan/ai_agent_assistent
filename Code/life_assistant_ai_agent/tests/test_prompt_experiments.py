# -*- coding: utf-8 -*-
"""
Prompt Engineering Experiment Script
====================================
æœ¬è„šæœ¬ç”¨äºå¯¹æ¯”ä¸åŒçš„Promptè®¾è®¡æ–¹æ¡ˆå’ŒOpenAI APIå‚æ•°å¯¹ç»“æœè´¨é‡çš„å½±å“ã€‚

å®éªŒç›®æ ‡ï¼š
1. æµ‹è¯•ä¸‰ç§ä¸åŒçš„Promptè®¾è®¡æ¨¡å¼ï¼š
   - baseline: åŸºç¡€è‡ªç”±æ–‡æœ¬æç¤º
   - structured: ç»“æ„åŒ–JSONæ¨¡æ¿æç¤º
   - function_calling: OpenAIå‡½æ•°è°ƒç”¨æ¨¡å¼
2. æµ‹è¯•ä¸åŒçš„APIå‚æ•°ç»„åˆ(temperature, top_p)å¯¹è¾“å‡ºç¨³å®šæ€§çš„å½±å“
3. è¯„ä¼°è¾“å‡ºçš„JSONæ ¼å¼æ­£ç¡®æ€§å’Œå†…å®¹å®Œæ•´æ€§

å®éªŒæ•°æ®æºï¼šé¡¹ç›®æ•°æ®åº“ä¸­çš„çœŸå®å¯¹è¯å†å²
è¯„ä¼°æŒ‡æ ‡ï¼šJSONè§£ææˆåŠŸç‡ã€å¿…éœ€å­—æ®µå®Œæ•´æ€§ã€è¾“å‡ºé•¿åº¦ä¸€è‡´æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
cd Code/life_assistant_ai_agent
python tests/test_prompt_experiments.py
"""

import os
import re
import csv
import json
import sqlite3
import itertools
from datetime import datetime
from pathlib import Path
import sys

# å¯¼å…¥ä¾èµ–åº“
from dotenv import load_dotenv
import openai

# ============================================================================
# 1. ç¯å¢ƒé…ç½®å’Œå¯¼å…¥é¡¹ç›®æ¨¡å—
# ============================================================================

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEY not found in .env file. Please add it first.")

# é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ–°ç‰ˆæœ¬APIæ ¼å¼ï¼‰
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# åŠ¨æ€å¯¼å…¥é¡¹ç›®é…ç½®ï¼ˆå¤„ç†ä¸åŒçš„è¿è¡Œç¯å¢ƒï¼‰
try:
    # å½“ä½œä¸ºæ¨¡å—è¿è¡Œæ—¶: python -m life_assistant_ai_agent.tests.test_prompt_experiments
    from ..config import DATABASE_PATH
except ImportError:
    # å½“ç›´æ¥è¿è¡Œæ—¶: python tests/test_prompt_experiments.py
    project_root = Path(__file__).resolve().parents[1]
    sys.path.insert(0, str(project_root))
    from config import DATABASE_PATH

# ============================================================================
# 2. å®éªŒé…ç½®å¸¸é‡
# ============================================================================

# å®éªŒå‚æ•°é…ç½®
N_MESSAGES_TO_FETCH = 30  # ä»æ•°æ®åº“æå–çš„å¯¹è¯æ¡æ•°
TARGET_USER_ID = 1        # ç›®æ ‡ç”¨æˆ·IDï¼ˆä½¿ç”¨mockæ•°æ®ä¸­çš„å¼ ä¸‰ï¼‰

# JSONè¾“å‡ºçš„å¿…éœ€å­—æ®µï¼ˆåŸºäºé¡¹ç›®éœ€æ±‚å®šä¹‰ï¼‰
REQUIRED_JSON_FIELDS = [
    "Main_Concerns",      # ä¸»è¦å…³æ³¨ç‚¹
    "Interests",          # å…´è¶£çˆ±å¥½  
    "Recent_Problems",    # è¿‘æœŸå›°æƒ‘
    "Action_Plans",       # è¡ŒåŠ¨è®¡åˆ’
    "Emotional_State"     # æƒ…ç»ªçŠ¶æ€
]

# è¾“å‡ºæ–‡ä»¶è·¯å¾„é…ç½®
RESULTS_DIR = Path(__file__).parent / "experiment_results"
RESULTS_DIR.mkdir(exist_ok=True)

RESULTS_CSV = RESULTS_DIR / "prompt_comparison_results.csv"
RAW_RESPONSES_DIR = RESULTS_DIR / "raw_responses"
RAW_RESPONSES_DIR.mkdir(exist_ok=True)

# ============================================================================
# 3. Promptæ¨¡æ¿å®šä¹‰ï¼ˆä½“ç°ä¸åŒçš„è®¾è®¡ç†å¿µï¼‰
# ============================================================================

# ç³»ç»Ÿè§’è‰²å®šä¹‰ï¼ˆæ‰€æœ‰å®éªŒå…±ç”¨ï¼‰
SYSTEM_PROMPT = """You are an intelligent life assistant specialized in helping international students and workers living in Japan. 
Your task is to analyze conversation history and extract key insights about the user's life situation.
Focus on themes related to study, work, visa issues, social life, health, and travel in Japan."""

# A. åŸºç¡€Promptï¼ˆå½“å‰é¡¹ç›®ä½¿ç”¨çš„ç®€å•æ¨¡å¼ï¼‰
BASELINE_TEMPLATE = """
Please analyze the following conversation history and summarize the user's main life aspects.

Conversation History:
{conversation_history}

Please provide a comprehensive summary covering the user's concerns, interests, problems, and plans.
"""

# B. ç»“æ„åŒ–JSON Promptï¼ˆæ˜ç¡®æŒ‡å®šè¾“å‡ºæ ¼å¼ï¼‰
STRUCTURED_JSON_TEMPLATE = """
Please analyze the following conversation history and extract key information about the user.

Conversation History:
{conversation_history}

You must output ONLY a valid JSON object with exactly these fields:
- "Main_Concerns": string describing the user's primary worries or focus areas
- "Interests": string listing the user's hobbies and interests  
- "Recent_Problems": string describing current challenges or difficulties
- "Action_Plans": string outlining the user's goals and planned actions
- "Emotional_State": string describing the user's general mood and attitude

Output format example:
{{
  "Main_Concerns": "Academic performance and visa renewal",
  "Interests": "Photography, basketball, AI technology",
  "Recent_Problems": "Language barrier in daily communication", 
  "Action_Plans": "Improve Japanese speaking skills, find part-time job",
  "Emotional_State": "Optimistic but sometimes stressed about deadlines"
}}

Output ONLY the JSON object, no additional text or explanation.
"""

# C. å‡½æ•°è°ƒç”¨æ¨¡å¼çš„Schemaå®šä¹‰ï¼ˆOpenAIåŸç”Ÿç»“æ„åŒ–è¾“å‡ºï¼‰
FUNCTION_CALLING_SCHEMA = {
    "name": "extract_user_insights",
    "description": "Extract and structure key insights about the user from conversation history",
    "parameters": {
        "type": "object",
        "properties": {
            "Main_Concerns": {
                "type": "string",
                "description": "User's primary worries, focus areas, or important life aspects"
            },
            "Interests": {
                "type": "string", 
                "description": "User's hobbies, interests, and enjoyable activities"
            },
            "Recent_Problems": {
                "type": "string",
                "description": "Current challenges, difficulties, or obstacles the user is facing"
            },
            "Action_Plans": {
                "type": "string",
                "description": "User's goals, planned actions, and future intentions"
            },
            "Emotional_State": {
                "type": "string",
                "description": "User's general mood, attitude, and emotional well-being"
            }
        },
        "required": REQUIRED_JSON_FIELDS
    }
}

# ============================================================================
# 4. æ•°æ®æå–å’Œé¢„å¤„ç†åŠŸèƒ½
# ============================================================================

def fetch_conversation_history(user_id: int, limit: int) -> str:
    """
    ä»SQLiteæ•°æ®åº“æå–æŒ‡å®šç”¨æˆ·çš„æœ€è¿‘å¯¹è¯å†å²
    
    Args:
        user_id: ç”¨æˆ·ID
        limit: æå–çš„æ¶ˆæ¯æ¡æ•°é™åˆ¶
        
    Returns:
        str: æ ¼å¼åŒ–çš„å¯¹è¯å†å²æ–‡æœ¬
    """
    try:
        with sqlite3.connect(DATABASE_PATH) as conn:
            cursor = conn.cursor()
            # æŸ¥è¯¢æœ€è¿‘çš„å¯¹è¯è®°å½•ï¼ŒæŒ‰æ—¶é—´å€’åº
            cursor.execute("""
                SELECT role, content, timestamp 
                FROM conversations 
                WHERE user_id = ? 
                ORDER BY id DESC 
                LIMIT ?
            """, (user_id, limit))
            
            conversations = cursor.fetchall()
            
        if not conversations:
            return "No conversation history found for this user."
            
        # æ ¼å¼åŒ–å¯¹è¯å†å²ï¼ˆæ—¶é—´æ­£åºæ’åˆ—ï¼‰
        formatted_history = []
        for role, content, timestamp in reversed(conversations):
            # è¿‡æ»¤æ‰ç³»ç»Ÿæç¤ºä¿¡æ¯
            if not (content.startswith("[User Profile]") or content.startswith("[Memory Summary]")):
                formatted_history.append(f"{role.capitalize()}: {content}")
                
        return "\n\n".join(formatted_history)
        
    except Exception as e:
        print(f"âŒ Error fetching conversation history: {e}")
        return ""

def clean_json_response(raw_text: str) -> str:
    """
    æ¸…ç†LLMè¾“å‡ºä¸­çš„å¤šä½™æ ¼å¼ï¼Œæå–çº¯JSONå†…å®¹
    
    Args:
        raw_text: LLMçš„åŸå§‹è¾“å‡ºæ–‡æœ¬
        
    Returns:
        str: æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
    """
    text = raw_text.strip()
    
    # ç§»é™¤markdownä»£ç å—æ ‡è®°
    code_block_pattern = r'```(?:json)?\s*(.*?)\s*```'
    match = re.search(code_block_pattern, text, re.DOTALL)
    if match:
        text = match.group(1).strip()
    
    # å¤„ç†è¢«å¼•å·åŒ…å›´çš„JSON
    if text.startswith('"') and text.endswith('"'):
        text = text[1:-1].replace('\\"', '"')
        
    return text

# ============================================================================
# 5. æ ¸å¿ƒå®éªŒæ‰§è¡Œå‡½æ•°
# ============================================================================

def execute_baseline_prompt(conversation_history: str, temperature: float, top_p: float) -> str:
    """
    æ‰§è¡ŒåŸºç¡€Promptå®éªŒ
    
    Args:
        conversation_history: å¯¹è¯å†å²æ–‡æœ¬
        temperature: éšæœºæ€§æ§åˆ¶å‚æ•°
        top_p: æ ¸é‡‡æ ·å‚æ•°
        
    Returns:
        str: LLMçš„åŸå§‹å“åº”æ–‡æœ¬
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": BASELINE_TEMPLATE.format(conversation_history=conversation_history)}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        max_tokens=800,  # ç»™äºˆè¶³å¤Ÿç©ºé—´è¾“å‡ºå®Œæ•´å†…å®¹
    )
    
    return response.choices[0].message.content

def execute_structured_prompt(conversation_history: str, temperature: float, top_p: float) -> str:
    """
    æ‰§è¡Œç»“æ„åŒ–JSON Promptå®éªŒ
    
    Args:
        conversation_history: å¯¹è¯å†å²æ–‡æœ¬
        temperature: éšæœºæ€§æ§åˆ¶å‚æ•°
        top_p: æ ¸é‡‡æ ·å‚æ•°
        
    Returns:
        str: LLMçš„åŸå§‹å“åº”æ–‡æœ¬
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": STRUCTURED_JSON_TEMPLATE.format(conversation_history=conversation_history)}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        max_tokens=800,
    )
    
    return response.choices[0].message.content

def execute_function_calling(conversation_history: str, temperature: float, top_p: float) -> str:
    """
    æ‰§è¡Œå‡½æ•°è°ƒç”¨æ¨¡å¼å®éªŒ
    
    Args:
        conversation_history: å¯¹è¯å†å²æ–‡æœ¬
        temperature: éšæœºæ€§æ§åˆ¶å‚æ•°
        top_p: æ ¸é‡‡æ ·å‚æ•°
        
    Returns:
        str: å‡½æ•°è°ƒç”¨è¿”å›çš„JSONå­—ç¬¦ä¸²
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Analyze this conversation history:\n\n{conversation_history}"}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=[{"type": "function", "function": FUNCTION_CALLING_SCHEMA}],
        tool_choice={"type": "function", "function": {"name": "extract_user_insights"}},
        temperature=temperature,
        top_p=top_p,
        max_tokens=800,
    )
    
    # æå–å‡½æ•°è°ƒç”¨çš„å‚æ•°ï¼ˆå·²ç»æ˜¯JSONæ ¼å¼ï¼‰
    tool_call = response.choices[0].message.tool_calls[0]
    return tool_call.function.arguments

# ============================================================================
# 6. ç»“æœè¯„ä¼°å’ŒæŒ‡æ ‡è®¡ç®—
# ============================================================================

def evaluate_response_quality(raw_response: str) -> dict:
    """
    è¯„ä¼°LLMå“åº”çš„è´¨é‡æŒ‡æ ‡
    
    Args:
        raw_response: LLMçš„åŸå§‹å“åº”
        
    Returns:
        dict: åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {
        "json_parseable": False,
        "required_fields_present": 0,
        "total_required_fields": len(REQUIRED_JSON_FIELDS),
        "response_length": len(raw_response),
        "empty_fields_count": 0
    }
    
    try:
        # å°è¯•è§£æJSON
        cleaned_json = clean_json_response(raw_response)
        parsed_data = json.loads(cleaned_json)
        metrics["json_parseable"] = True
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µçš„å­˜åœ¨æ€§å’Œå®Œæ•´æ€§
        for field in REQUIRED_JSON_FIELDS:
            if field in parsed_data:
                metrics["required_fields_present"] += 1
                # æ£€æŸ¥å­—æ®µæ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦
                if not parsed_data[field] or not parsed_data[field].strip():
                    metrics["empty_fields_count"] += 1
                    
    except (json.JSONDecodeError, KeyError, TypeError) as e:
        # JSONè§£æå¤±è´¥æˆ–å­—æ®µè®¿é—®é”™è¯¯
        pass
        
    # è®¡ç®—å­—æ®µå®Œæ•´ç‡
    metrics["field_completeness_rate"] = metrics["required_fields_present"] / metrics["total_required_fields"]
    
    return metrics

def save_raw_response(response_text: str, experiment_id: str, prompt_type: str, 
                     temperature: float, top_p: float) -> str:
    """
    ä¿å­˜LLMçš„åŸå§‹å“åº”åˆ°æ–‡ä»¶
    
    Args:
        response_text: LLMå“åº”æ–‡æœ¬
        experiment_id: å®éªŒæ‰¹æ¬¡ID
        prompt_type: Promptç±»å‹
        temperature: æ¸©åº¦å‚æ•°
        top_p: top_på‚æ•°
        
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    filename = f"{experiment_id}_{prompt_type}_temp{temperature}_top_p{top_p}.txt"
    filepath = RAW_RESPONSES_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"Experiment: {experiment_id}\n")
        f.write(f"Prompt Type: {prompt_type}\n")
        f.write(f"Temperature: {temperature}\n")
        f.write(f"Top P: {top_p}\n")
        f.write(f"Timestamp: {datetime.now().isoformat()}\n")
        f.write("="*60 + "\n")
        f.write(response_text)
        
    return str(filepath)

# ============================================================================
# 7. ä¸»å®éªŒæµç¨‹æ§åˆ¶
# ============================================================================

def run_comprehensive_experiment():
    """
    è¿è¡Œå®Œæ•´çš„Prompt Engineeringå¯¹æ¯”å®éªŒ
    """
    print("ğŸš€ Starting Prompt Engineering Experiment")
    print("="*60)
    
    # ç”Ÿæˆå®éªŒæ‰¹æ¬¡ID
    experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"ğŸ“‹ Experiment ID: {experiment_id}")
    
    # è·å–å®éªŒæ•°æ®
    print(f"ğŸ“Š Fetching conversation history for user {TARGET_USER_ID}...")
    conversation_history = fetch_conversation_history(TARGET_USER_ID, N_MESSAGES_TO_FETCH)
    
    if not conversation_history or conversation_history.startswith("No conversation"):
        print("âŒ No valid conversation history found. Please check your database.")
        return
        
    print(f"âœ… Retrieved {len(conversation_history.split())} words of conversation history")
    
    # å®šä¹‰å®éªŒå‚æ•°ç½‘æ ¼
    prompt_types = ["baseline", "structured", "function_calling"]
    temperature_values = [0.0, 0.3, 0.7]  # ä½ã€ä¸­ã€é«˜éšæœºæ€§
    top_p_values = [0.8, 1.0]             # ä¿å®ˆã€å¼€æ”¾çš„é‡‡æ ·
    
    total_experiments = len(prompt_types) * len(temperature_values) * len(top_p_values)
    print(f"ğŸ”¬ Running {total_experiments} experiments...")
    
    # å‡†å¤‡ç»“æœCSVæ–‡ä»¶
    csv_headers = [
        "experiment_id", "prompt_type", "temperature", "top_p",
        "json_parseable", "required_fields_present", "total_required_fields",
        "field_completeness_rate", "response_length", "empty_fields_count",
        "raw_file_path", "timestamp"
    ]
    
    # å†™å…¥CSVå¤´éƒ¨ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if not RESULTS_CSV.exists():
        with open(RESULTS_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(csv_headers)
    
    # æ‰§è¡Œå®éªŒç½‘æ ¼
    experiment_count = 0
    for prompt_type, temperature, top_p in itertools.product(prompt_types, temperature_values, top_p_values):
        experiment_count += 1
        print(f"\n[{experiment_count}/{total_experiments}] Testing: {prompt_type} | temp={temperature} | top_p={top_p}")
        
        try:
            # æ ¹æ®promptç±»å‹æ‰§è¡Œç›¸åº”çš„å®éªŒ
            if prompt_type == "baseline":
                response = execute_baseline_prompt(conversation_history, temperature, top_p)
            elif prompt_type == "structured":
                response = execute_structured_prompt(conversation_history, temperature, top_p)
            elif prompt_type == "function_calling":
                response = execute_function_calling(conversation_history, temperature, top_p)
            else:
                raise ValueError(f"Unknown prompt type: {prompt_type}")
            
            # è¯„ä¼°å“åº”è´¨é‡
            metrics = evaluate_response_quality(response)
            
            # ä¿å­˜åŸå§‹å“åº”
            raw_file_path = save_raw_response(response, experiment_id, prompt_type, temperature, top_p)
            
            # è®°å½•ç»“æœåˆ°CSV
            with open(RESULTS_CSV, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    experiment_id, prompt_type, temperature, top_p,
                    metrics["json_parseable"], metrics["required_fields_present"], 
                    metrics["total_required_fields"], metrics["field_completeness_rate"],
                    metrics["response_length"], metrics["empty_fields_count"],
                    raw_file_path, datetime.now().isoformat()
                ])
            
            # æ˜¾ç¤ºå®æ—¶ç»“æœ
            status = "âœ… PASS" if metrics["json_parseable"] else "âŒ FAIL"
            completeness = f"{metrics['required_fields_present']}/{metrics['total_required_fields']}"
            print(f"   Result: {status} | Fields: {completeness} | Length: {metrics['response_length']} chars")
            
        except Exception as e:
            print(f"   âŒ Error: {str(e)}")
            # è®°å½•é”™è¯¯åˆ°CSV
            with open(RESULTS_CSV, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    experiment_id, prompt_type, temperature, top_p,
                    False, 0, len(REQUIRED_JSON_FIELDS), 0.0, 0, 0,
                    f"ERROR: {str(e)}", datetime.now().isoformat()
                ])
    
    print("\n" + "="*60)
    print("ğŸ‰ Experiment completed successfully!")
    print(f"ğŸ“ Results saved to: {RESULTS_CSV}")
    print(f"ğŸ“„ Raw responses saved to: {RAW_RESPONSES_DIR}")
    print("\nğŸ“ˆ To analyze results, you can:")
    print(f"   â€¢ Open {RESULTS_CSV} in Excel or Google Sheets")
    print(f"   â€¢ Check individual responses in {RAW_RESPONSES_DIR}")

# ============================================================================
# 8. è„šæœ¬å…¥å£ç‚¹
# ============================================================================

if __name__ == "__main__":
    try:
        run_comprehensive_experiment()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Experiment interrupted by user")
    except Exception as e:
        print(f"\n\nğŸ’¥ Unexpected error: {e}")
        import traceback
        traceback.print_exc() 